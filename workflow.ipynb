{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Workflow for `DESS`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Importing custom utilities\n",
    "from stats import *\n",
    "from data_pipeline_manager import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = 'storage/input.dta'\n",
    "UPLOAD_FILE_PATH = 'storage/completed_DepartmenttoSearch_September2024.dta'\n",
    "COMPLETE_FILE_PATH = 'storage/complete.parquet'\n",
    "REPROCESS_FILE_PATH = 'storage/reprocess.parquet'\n",
    "UNCOMPLETE_FILE_PATH = 'storage/uncomplete.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prereprocessing\n",
    "\n",
    "We recieve a file from Dropbox that we pass through our data pipeline to merge into our internal existing parquet files. Also, in this stage, we add relevant columns to `uncompleted.parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master = pd.read_stata(INPUT_FILE)\n",
    "df_c = pd.read_parquet(COMPLETE_FILE_PATH)\n",
    "df_r = pd.read_parquet(REPROCESS_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "| Total    : 238880  |\n",
      "| Complete : 212454  |\n",
      "| Reprocess: 932     |\n",
      "|--------------------|\n",
      "| ToDo     : 25494   |\n",
      "+--------------------+\n"
     ]
    }
   ],
   "source": [
    "get_expected_file_split_stats(df_master, df_c, df_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_u = get_new_rows()\n",
    "df_u\n",
    "# append_to_file(UNCOMPLETE_FILE_PATH, df_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_u = add_dess_columns(df_u)\n",
    "df_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data\n",
    "Current thinking is that the three high-level functions should be modularized since they may be used in any order.\n",
    "\n",
    "### Scraping\n",
    "- Run the processing script on `uncompleted.parquet`.\n",
    "    - This can be executed either outside the notebook (eg `./main.py`) or inside the notebook (eg: `!caffeinate -dui ./main.py`)\n",
    "    - You would probably want some way to `get_stats` about the processing (look at `statusUpdate` from `status.ipynb`)\n",
    "    - Relevant function(s) for this: `process_and_cache(df)`\n",
    "- Filter out relevant rows and add to `reprocessed.parquet` (based on null rawText).\n",
    "\n",
    "### Department Extraction\n",
    "- Call `populate_faculty` on file \n",
    "    - By default, the file will be `uncompleted.parquet`.\n",
    "    - It could also be on `completed.parquet`— when we make changes to our department extraction logic (and want to re-run it on the existing data).\n",
    "\n",
    "### Data merging\n",
    "- Want to merge the scraped + faculty-filled information to other internal files\n",
    "    - Add rows to `completed.parquet` and `reprocessed.parquet`\n",
    "    - Remove rows from `uncompleted.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Processing\n",
    "- Get a status update about the dataset\n",
    "    - Want to the completion rates & conversion rates (look at `final_merge.ipynb`?)\n",
    "- Prepare the completed file for upload\n",
    "    - convert `completed.parquet` to `.dta` (for uploading to Dropbox)\n",
    "    - Need to think about how to handle file synchronization and data redundancy for these internal files— easiest solution is push these parquet files to dropbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other ToDos\n",
    "- Update the department extraction logic:\n",
    "    - Re-order the primary patterns (move \"the\" pattern to top)\n",
    "    - Add logic to handle isProfessor2 variable\n",
    "    - Figure out what edge cases to consider"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dessVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
