{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `DESS`: End-to-End Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Importing custom utilities\n",
    "import stats as stats\n",
    "import data_pipeline_manager as dpm\n",
    "import dess.search as search\n",
    "import dess.nlp as nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = 'storage/input.dta'\n",
    "UPLOAD_FILE_PATH = 'storage/completed_DepartmenttoSearch_November2024.dta'\n",
    "COMPLETE_FILE_PATH = 'storage/complete.parquet'\n",
    "REPROCESS_FILE_PATH = 'storage/reprocess.parquet'\n",
    "UNCOMPLETE_FILE_PATH = 'storage/uncomplete.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prereprocessing\n",
    "\n",
    "We recieve a file from Dropbox that we pass through our data pipeline to merge into our internal existing parquet files. Also, in this stage, we add relevant columns to `uncompleted.parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master = pd.read_stata(INPUT_FILE)\n",
    "df_c = pd.read_parquet(COMPLETE_FILE_PATH)\n",
    "df_r = pd.read_parquet(REPROCESS_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.get_expected_file_split_stats(df_master, df_c, df_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_u = dpm.get_new_rows()\n",
    "df_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_u = dpm.prepare_dess_data_structure(df_u)\n",
    "df_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpm.write_to_file(UNCOMPLETE_FILE_PATH, df_u, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data\n",
    "In our existing workflow, we have three high-level functions that should be modularized since they may be used in any order. These are:\n",
    "- **Scrape** — search for faculty information (from Google)\n",
    "- **Extract** — Based on snapshots from Google search results, extract relevant faculty information.\n",
    "- **Merge** — Update our files with new information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping\n",
    "Run the processing script on `uncompleted.parquet`. Code block included below is meant for demonstrations purposes. It's preferable to run this process in a separate terminal window to easily track processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "caffeinate -dui python3 search.py [start_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the progress of the scraping script, run the following block. It will provide an update on number of chunks processed, and percentage of entire file processed so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_u = pd.read_parquet(UNCOMPLETE_FILE_PATH)\n",
    "stats.get_chunk_processing_stats(df_u, CHUNK_SIZE=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Department Extraction\n",
    "- Call `populate_faculty` on file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.extract_department_information(df_u) # modifes relevant columns in place\n",
    "df_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data merging\n",
    "Often the scraping process is run in parallel so we have utilites to stitch together the complete file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_u_full = dpm.get_merged_data_from_parallel_scrape(pd.read_parquet('storage/uncomplete-akhil.parquet'),\n",
    "                                                 pd.read_parquet('storage/uncomplete-shishir.parquet'))\n",
    "dpm.write_to_file(UNCOMPLETE_FILE_PATH, df_u_full, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to merge the scraped and faculty-filled information to other internal files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add rows to `completed.parquet` and `reprocessed.parquet` and remove rows from `uncompleted.parquet`\n",
    "# Will provide error messages for any conflicts. Returns the updated dataframes\n",
    "df_c, df_r = dpm.update_internal_files(df_c, df_r, df_u_full)\n",
    "\n",
    "# TODO: write back\n",
    "dpm.write_to_file(COMPLETE_FILE_PATH, df_c, overwrite=True)\n",
    "dpm.write_to_file(REPROCESS_FILE_PATH, df_r, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Processing\n",
    "To geta status update about the dataset—i.e. to get an overview of the completion rates and conversion rates—run the following block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.read_parquet(COMPLETE_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.get_dataset_stats('storage/result.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To backup the results (i.e. upload to Dropbox), run the following blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpm.create_stata_output_file(\"completeNov.dta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpm.orchestrate_upload_workflow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dessVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
