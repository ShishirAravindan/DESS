{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Workflow for `DESS`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prereprocessing\n",
    "\n",
    "We recieve a file from Dropbox that we convert to parquet and merge to our internal existing files (`completed.parquet`, `uncompleted.parquet` and `reprocessed.parquet`). \n",
    "- Convert `.dta` file to parquet\n",
    "- Based on fullid column, check if it's already been processed (check in `reprocessed.parquet` and `completed.parquet`).\n",
    "    - If not, add it to `uncompleted.parquet`.\n",
    "- Add relevant columns to `uncompleted.parquet` (see `prune_dataframe` function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data\n",
    "Current thinking is that the three high-level functions should be modularized since they may be used in any order.\n",
    "\n",
    "### Scraping\n",
    "- Run the processing script on `uncompleted.parquet`.\n",
    "    - This can be executed either outside the notebook (eg `./main.py`) or inside the notebook (eg: `!caffeinate -dui ./main.py`)\n",
    "    - You would probably want some way to `get_stats` about the processing (look at `statusUpdate` from `status.ipynb`)\n",
    "    - Relevant function(s) for this: `process_and_cache(df)`\n",
    "- Filter out relevant rows and add to `reprocessed.parquet` (based on null rawText).\n",
    "\n",
    "### Department Extraction\n",
    "- Call `populate_faculty` on file \n",
    "    - By default, the file will be `uncompleted.parquet`.\n",
    "    - It could also be on `completed.parquet`— when we make changes to our department extraction logic (and want to re-run it on the existing data).\n",
    "\n",
    "### Data merging\n",
    "- Want to merge the scraped + faculty-filled information to other internal files\n",
    "    - Add rows to `completed.parquet` and `reprocessed.parquet`\n",
    "    - Remove rows from `uncompleted.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Processing\n",
    "- Get a status update about the dataset\n",
    "    - Want to the completion rates & conversion rates (look at `final_merge.ipynb`?)\n",
    "- Prepare the completed file for upload\n",
    "    - convert `completed.parquet` to `.dta` (for uploading to Dropbox)\n",
    "    - Need to think about how to handle file synchronization and data redundancy for these internal files— easiest solution is push these parquet files to dropbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other ToDos\n",
    "- Update the department extraction logic:\n",
    "    - Re-order the primary patterns (move \"the\" pattern to top)\n",
    "    - Add logic to handle isProfessor2 variable\n",
    "    - Figure out what edge cases to consider"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
